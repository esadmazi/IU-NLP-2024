{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NORVIG'S SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['value']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "\n",
        "def correction(sentence):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    result = []\n",
        "    for word in sentence:\n",
        "        result.append(max(candidates(word), key=P))\n",
        "    return result\n",
        "\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "correction(['valuu'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Problem with Norvig's solution:**\n",
        "It is not context sensitive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "class CS_corrector:\n",
        "    def __init__(self, ngram):\n",
        "        self.ngram_file = ngram\n",
        "        self.calc_words()\n",
        "        self.conditional_probs = self.calc_conditional_probs()\n",
        "\n",
        "    def calc_conditional_probs(self):\n",
        "        '''\n",
        "            To calculate the conditional probability from the ngram model\n",
        "        '''\n",
        "        conditional_probs = {}\n",
        "        for two_words, count_w1_w2 in self.w1_w2_counter.items():\n",
        "            w1, _ = two_words.split('_')\n",
        "            conditional_probs[two_words] = int(\n",
        "                count_w1_w2) / self.w1_counter[w1]\n",
        "        return conditional_probs\n",
        "\n",
        "    def calc_words(self):\n",
        "        '''\n",
        "            Collecting the information needed to build the ngram model\n",
        "        '''\n",
        "        self.words = []\n",
        "        self.w1_w2_counter = {}\n",
        "        self.w1_counter = {}\n",
        "        with open(self.ngram_file, 'r', encoding='latin-1') as f:\n",
        "            for line in f.readlines():\n",
        "                count, w1, w2 = line.split()\n",
        "                self.w1_w2_counter[w1+'_'+w2] = count\n",
        "                if self.w1_counter.get(w1):\n",
        "                    self.w1_counter[w1] += int(count)\n",
        "                else:\n",
        "                    self.w1_counter[w1] = int(count)\n",
        "                self.words.append(w1)\n",
        "                self.words.append(w2)\n",
        "        self.words = Counter(self.words)\n",
        "\n",
        "    def edit_distance(self, word1, word2):\n",
        "        \"\"\"Calculate the Levenshtein distance between two words.\"\"\"\n",
        "        if len(word1) < len(word2):\n",
        "            return self.edit_distance(word2, word1)\n",
        "\n",
        "        if not word2:\n",
        "            return len(word1)\n",
        "\n",
        "        previous_row = range(len(word2) + 1)\n",
        "\n",
        "        for i, c1 in enumerate(word1):\n",
        "            current_row = [i + 1]\n",
        "\n",
        "            for j, c2 in enumerate(word2):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "\n",
        "            previous_row = current_row\n",
        "        if previous_row[-1] != 0:\n",
        "            return previous_row[-1]\n",
        "        return 1\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"All edits that are one edit away from `word`.\"\n",
        "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word): return (e2 for e1 in self.edits1(word)\n",
        "                                    for e2 in self.edits1(e1))\n",
        "\n",
        "    def known(self, words): return set(w for w in words if w in self.words)\n",
        "\n",
        "    def candidates(self, word):\n",
        "        '''\n",
        "            Getting all the candidate words that can be the correction of word\n",
        "        '''\n",
        "        if len(word) == 1:\n",
        "            return word\n",
        "        l = []\n",
        "        l.extend(self.known(self.edits1(word)))\n",
        "        if len(word) > 4:\n",
        "            l.extend(self.known(self.edits2(word)))\n",
        "        l.append(word)\n",
        "        return l\n",
        "\n",
        "    def get_best_word(self, original_word, cands, next_word=None, previous_word=None):\n",
        "        '''\n",
        "            Choosing the best word from the candidates list\n",
        "        '''\n",
        "        best_word = None\n",
        "        best_score = 0\n",
        "        conditional_probs_w = 0.9\n",
        "        frequency_w = 0.1\n",
        "        edit_distance_w = 0.8\n",
        "\n",
        "        if next_word or previous_word:\n",
        "            for word in cands:\n",
        "                gram_format = f'{word}_{next_word}' if next_word else f'{previous_word}_{word}'\n",
        "                if self.conditional_probs.get(gram_format):\n",
        "                    score = frequency_w * self.words[word]/len(self.words) + \\\n",
        "                        edit_distance_w * (1/self.edit_distance(word, original_word)) + \\\n",
        "                        conditional_probs_w * \\\n",
        "                        self.conditional_probs[gram_format]\n",
        "\n",
        "                    if word == original_word and self.known([word]):\n",
        "                        score += 0.1\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_word = word\n",
        "\n",
        "        if best_word == None:\n",
        "            # choose the most frequent candidate with the least edit distance\n",
        "            for word in cands:\n",
        "                if self.words[word] > 0:\n",
        "                    score = 0.8 * self.words[word]/len(self.words) + \\\n",
        "                        0.2 * 1/(self.edit_distance(word, original_word))\n",
        "                    if word == original_word and self.known([word]):\n",
        "                        score += 0.1\n",
        "                    if score > best_score:\n",
        "                        best_word = word\n",
        "                        best_score = score\n",
        "        if best_word == None:\n",
        "            return original_word\n",
        "        return best_word\n",
        "\n",
        "    def check(self, sentence):\n",
        "        '''\n",
        "            This function takes the sentence as an input and return the same sentence corrected\n",
        "        '''\n",
        "        result = []\n",
        "        # correcting the first word\n",
        "        first_word = sentence[0]\n",
        "        if len(sentence) > 1:\n",
        "            if self.words[first_word] <= 50:  # mean that this word need correction\n",
        "                next_word = sentence[1]\n",
        "                cands = self.candidates(first_word)\n",
        "                best_word = self.get_best_word(first_word, cands, next_word)\n",
        "                result.append(best_word)\n",
        "            else:\n",
        "                result.append(first_word)\n",
        "        else:\n",
        "            cands = self.candidates(first_word)\n",
        "            return self.get_best_word(first_word, cands)\n",
        "\n",
        "        # correcting the rest of the sentence\n",
        "        for i in range(1, len(sentence)):\n",
        "            # if the word frequency is less than 50, then it is probably misspelled\n",
        "            if self.words[sentence[i]] <= 50:\n",
        "                cands = self.candidates(sentence[i])\n",
        "                previous_word = result[i-1]\n",
        "                best_word = self.get_best_word(\n",
        "                    sentence[i], cands, previous_word=previous_word)\n",
        "                result.append(best_word)\n",
        "            else:\n",
        "                result.append(sentence[i])\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['english', 'is', 'a', 'dying', 'language']\n",
            "['the', 'men', 'are', 'doing', 'sports']\n"
          ]
        }
      ],
      "source": [
        "my_corrector = CS_corrector('bigrams.txt')\n",
        "print(my_corrector.check([\"English\", \"is\", \"a\", \"dming\", \"language\"]))\n",
        "print(my_corrector.check([\"The\", \"men\", \"are\", \"dming\", \"sports\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can clearly see, our program can grasp the context and answer correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "1. Bigrams are chosen for spell correction because they provide a balance between capturing contextual information and managing computational complexity. They offer reasonable accuracy while requiring less data and computational resources compared to higher-order n-grams.\n",
        "\n",
        "2. Edit Distance Weights: Weights are assigned to factors like frequency, edit distance, and conditional probability to balance their influence on correction decisions. Instead of running long epochs, we brute forced weights due to limited time for the deadline.\n",
        "\n",
        "3. Candidate Selection: Candidate words are generated based on common edit operations and favor known words from the dataset.\n",
        "\n",
        "4. Scoring and Word Selection: Words are scored based on multiple factors, including contextual information, to determine the best correction.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***I could not create a test set and evaluate, due to limited deadlines.***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
